# Maintained by Ansible
#listeners=PLAINTEXT://:9092
listeners=104.130.220.120:9092,104.130.220.119:9092,104.130.220.122:9092
main_nodes_ips_with_port: "{% set IP_ARR=[] %}{% for host in groups['mainnodes'] %}{% if IP_ARR.insert(loop.index,hostvars[host]['ansible_ssh_host']) %}{% endif %}{% endfor %}{{IP_ARR|join(':3000,')}


zookeeper.connect=104.130.220.120:2181,104.130.220.119:2181,104.130.220.122:2181
main_nodes_ips_with_port: "{% set IP_ARR=[] %}{% for host in groups['mainnodes'] %}{% if IP_ARR.insert(loop.index,hostvars[host]['ansible_ssh_host']) %}{% endif %}{% endfor %}{{IP_ARR|join(':3000,')}

log.dirs=/kafka/data
change to proper mount
broker.id=1
{ broker_id }

log.segment.bytes=1073741824
socket.receive.buffer.bytes=102400
socket.send.buffer.bytes=102400
confluent.metrics.reporter.topic.replicas=3
num.network.threads=8
ssl.endpoint.identification.algorithm=
num.io.threads=16
confluent.metrics.reporter.ssl.endpoint.identification.algorithm=
transaction.state.log.min.isr=2
zookeeper.connection.timeout.ms=6000
offsets.topic.replication.factor=3
socket.request.max.bytes=104857600
log.retention.check.interval.ms=300000
group.initial.rebalance.delay.ms=0
#metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter
num.recovery.threads.per.data.dir=2
transaction.state.log.replication.factor=3
#confluent.metrics.reporter.bootstrap.servers=104.130.220.116:9092
log.retention.hours=168
num.partitions=1

# Confluent Support
#confluent.support.metrics.enable=true
#confluent.support.customer.id=anonymous
